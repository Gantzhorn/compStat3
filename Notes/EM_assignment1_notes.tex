\documentclass[a4paper, 11 pt]{article}
\usepackage{datetime}
\usepackage{lipsum}
\setlength{\columnsep}{25 pt}
\usepackage{tabularx,booktabs,caption}
\usepackage{multicol,tabularx,capt-of}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{blkarray}
\usepackage[margin=1.2in]{geometry}
\usepackage{amsthm, amsmath, amssymb, amsfonts, commath, amsthm}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{listings}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage[danish]{babel}
%\renewcommand{\qedsymbol}{\textit{Q.E.D}}
\newtheorem{theorem}{Theorem}[section]
\addto\captionsdanish{\renewcommand\proofname{Proof}}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{corollary}[section]
\usepackage{fancyhdr}
\usepackage{titlesec}
\newcommand{\code}[1]{\texttt{#1}}
\lhead{Anders Gantzhorn - tzk942}
\chead{}
\rhead{09-11-2022}
\pagestyle{fancy}
\setlength{\headheight}{15pt}
\cfoot{\thepage\ af \pageref{LastPage}}
\setcounter{section}{0}
\title{EM algorithm for $t$-distribution}
\author{Anders G. Kristensen}
\date{09-11-2022}
\begin{document}
\maketitle
\section{Full model and likelihood}
\noindent Given $Y = (X,W)\in\mathbb{R} \times (0,\infty)$ with joint density 
\[
    f(x,w) = \frac{1}{\sqrt{\pi\nu\sigma^2}2^{(\nu+1)}\Gamma(\frac{\nu}{2})}w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)}
\]
With location parameter, $\mu\in\mathbb{R}$, scale parameter $\sigma>0$ and shape parameter $\nu>0$. 
\begin{theorem}
    $X$ has the $t$-distribution as its marginal density
\end{theorem}
\begin{proof}
    By definition the marginal density of $X$ is given as
    \begin{align*}
        g(x) &= \int f(x,w) dw = \int \frac{1}{\sqrt{\pi\nu\sigma^2}2^{(\nu+1)/2}\Gamma(\frac{\nu}{2})}w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)} dw \\
        &= \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\pi\nu\sigma^2}2^{(\nu+1)/2}\Gamma(\frac{\nu}{2})\left(\frac{1}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)\right)^{(\frac{\nu+1}{2})}} \int \frac{\frac{1}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)^{(\frac{\nu+1}{2})}}{\Gamma(\frac{\nu+1}{2})} w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)}dw\\
        &= \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\pi\nu\sigma^2}\Gamma(\frac{\nu}{2})\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)^{(\frac{\nu+1}{2})}} = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sqrt{\pi\nu\sigma^2}}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)^{-\left(\frac{\nu+1}{2}\right)}
    \end{align*}
\end{proof}
\noindent Consider the likelihood of the full data.
\begin{align*}
    &\sum_{i = 1}^N \log\left(f(x_i,w_i)\right) = \sum_{i = 1}^N \log\left(\frac{1}{\sqrt{\pi\nu\sigma^2}2^{(\nu+1)}\Gamma(\frac{\nu}{2})}w_i^{\frac{\nu-1}{2}}e^{-\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2}\right)}\right) \\
    &= \sum_{i = 1}^N -\frac{1}{2}\log\left(\pi\nu\sigma^2\right) - \left(\nu+1\right)\log\left(2\right)-\log\left(\Gamma\left(\frac{\nu}{2}\right)\right) + \log\left(w_i\right)\left(\frac{\nu-1}{2}\right)-\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2}\right) \\
    &= -\frac{N}{2}\log\left(\pi\nu\sigma^2\right) - N\left(\nu+1\right)\log\left(2\right)-N\log\left(\Gamma\left(\frac{\nu}{2}\right)\right) + \sum_{i = 1}^N \log\left(w_i\right)\left(\frac{\nu-1}{2}\right)-\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2}\right)
\end{align*}
To get the maximum-likelihood estimator we set the gradient of the likelihood equal to zero. First we calculate the entries
\begin{align}
    \frac{\partial}{\partial \sigma^2} \sum_{i = 1}^N \log\left(f(x_i,w_i)\right) = -\frac{N}{2\sigma^2} + \sum_{i = 1}^N \frac{w_i(x_i-\mu)^2}{2\nu\sigma^4} \label{eq1}
\end{align}
And 
\begin{align}
    \frac{\partial}{\partial \mu} \sum_{i = 1}^N \log\left(f(x_i,w_i)\right) = \sum_{i = 1}^N \frac{w_i(x_i-\mu)}{\nu \sigma^2} \label{eq2}
\end{align}
Setting (\ref{eq2}) to zero gives
\begin{align*}
    \sum_{i = 1}^N w_i(x_i-\mu) &= 0 \\
    \mu = \bar{wx}
\end{align*}
And from (\ref{eq2}) we see
\begin{align*}
    0 &= - N + \sum_{i = 1}^N \frac{w_i(x_i-\mu)^2}{\nu\sigma^2}\\
    \sigma^2 &= \frac{1}{\nu N} \sum_{i = 1}^N w_i(x_i-\mu)^2
\end{align*}
We are going to need the conditional distribution $W|X$. We see that $W|X$ is $\propto f(x,w)$, whence we see that $W|X\sim \Gamma\left(\frac{\nu+1}{2}, \frac{1}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)\right)$. Note also that this implies 
\[
    E\left[W|X\right] = \frac{\frac{\nu+1}{2}}{\frac{1}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)}, \qquad E\left[\log\left(W\right)|X\right] = -\log\left(\frac{1}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)\right) + \psi\left(\frac{\nu+1}{2}\right)
\]
\section{The EM-algorithm}
To implement the EM-algorithm we calculate the Q-function
\begin{align*}
    Q(\theta|\theta') &= E_{\theta'}\left[\log\left(f(x,w)\right) \middle| X = x \right] \\&= -\frac{1}{2}\log\left(\pi\nu\sigma^2\right) +  E_{\theta'}\left[ \log\left(w\right) \middle| X = x \right] \left(\frac{\nu-1}{2}\right) - \frac{1}{2}E_{\theta'}\left[ w \middle| X = x \right] \left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)\\
\end{align*}
Where we have removed the terms that don't depend on $\sigma^2, \mu$. Differentiating w.r.t. $\sigma^2, \mu$ yields
\[
    \frac{\partial}{\partial \mu}\left(Q(\theta|\theta')\right) = E_{\theta'}\left[ w \middle| X = x \right]\frac{(x-\mu)}{\nu\sigma^2}
\]
\[
    \frac{\partial}{\partial \sigma^2}\left(Q(\theta|\theta')\right) = -\frac{1}{2\sigma^2} +\frac{1}{2}E_{\theta'}\left[ w \middle| X = x \right] \frac{(x-\mu)^2}{\nu\sigma^4}
\]
Setting them each equal to zero gives
\begin{align*}
    \frac{\partial}{\partial \mu}\left(Q(\theta|\theta')\right) &= 0 \\
    \hat{\mu} = \frac{\sum_{i = 1}^N x_i E_{\theta'}\left[ w_i \middle| X = x_i \right]}{\sum_{i = 1}^N E_{\theta'}\left[ w_i \middle| X = x_i \right]}
\end{align*}
And 
\begin{align*}
    \frac{\partial}{\partial \sigma^2}\left(Q(\theta|\theta')\right) &= 0 \\
    N &= \sum_{i = 1}^N E_{\theta'}\left[ w_i \middle| X = x_i \right] \frac{(x_i-\mu)^2}{\nu\sigma^2}\\
    \hat{\sigma}^2 &= \frac{1}{N\nu} \sum_{i = 1}^N  E_{\theta'}\left[ w_i \middle| X = x_i \right](x_i-\mu)^2 
\end{align*}
\section{The Fisher-information}
We find the fisher-information theoretically first
\[
    \frac{\partial}{\partial \mu^2}\left(Q(\theta|\theta')\right) = -\frac{E_{\theta'}\left[ w \middle| X = x \right]}{\nu\sigma^2} 
\]
\[
    \frac{\partial}{\partial \mu\sigma^2}\left(Q(\theta|\theta')\right) = -\frac{E_{\theta'}\left[ w \middle| X = x \right](x-\mu)}{\nu\sigma^2}
\]
\[
    \frac{\partial}{\partial (\sigma^2)^2}\left(Q(\theta|\theta')\right) = \frac{1}{2\sigma^4} - E_{\theta'}\left[ w \middle| X = x \right] \frac{(x-\mu)^2}{\nu \sigma^6}
\]
\section{Estimation of $\nu$}
To estimate $nu$ we revisit the likelihood of the full data. 
\begin{align*}
    \sum_{i = 1}^N \log\left(f(x_i,w_i)\right) = -\frac{N}{2}\log\left(\pi\nu\sigma^2\right) - N\left(\nu+1\right)\log\left(2\right)-N\log\left(\Gamma\left(\frac{\nu}{2}\right)\right) \\ + \sum_{i = 1}^N \log\left(w_i\right)\left(\frac{\nu-1}{2}\right)-\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2}\right)      
\end{align*}
\end{document}